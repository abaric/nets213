Amelia Goodman, Andrew Graffy, Andrea Baric

NETS 213 - Homework 6

26 February 2016

#Financial Incentives with Crowdworkers

####Crowdsourcing Literature Review

The first crowdsourcing study we reviewed was “Labeling Images with a Computer Game” by Luis von Ahn and Laura Dabbish. This study tests the idea that instead of using computers to classify and label images, if the study creates a game that is entertaining enough, people will (kind of unknowingly) contribute to a huge crowdsourcing effort to label all the images on the internet. There are millions of images on the internet, and before this study, most techniques to categorize the images use the contents of the text appearing on the page, which is ineffective and leaves many gaps. The labeling of these images has important implications, like helping to describe images aloud to visually impaired people, web search, and machine learning. Their experiment hinges on the game being fun enough that people want to play it (and therefore somewhat unknowingly contribute to their project of labeling images).

The game works as follows: two random players are matched together. They can’t communicate and don’t know the other player is. They are both shown the same image, and must type words on the screen to try to “agree” on a word. They “agree” when they both type and enter the same word, which is a match and gains them points. Each player cannot see what the other is typing, and can’t type any “taboo words” (which are determined by previous matches). The taboo words allows for more diverse labels, and people typically only “agree” on accurate labels for the images. Players can “pass” an image only if both agree to do so, and after certain images are passed multiple times, they’re removed from the game because it is no longer fun to play. The experiment also made quite an extensive attempt to error check, in cheating, to image selection, to spell checking.

Through different approaches of confirming their results, the authors demonstrated that the labels typically were 83-86% precise. This is a huge success for the experiment, because they predict if 5,000 people play the game for 24 hours a day, every image on Google could be labeled properly within weeks. This model also creates a very pivotal realization for crowdsourcing information: if the game is fun, you can have a huge increase in participants who want to do well because of the competitive nature of the game, and no payments must be made.


The second paper we reviewed was “Crowds in Two Seconds” by Bernstein, Brandt, Miller, and Karger. People often associate crowdsourcing with the “retainer model,” where one pays workers a low wage and waits for their response. The “rapid refinement” model instead has crowdworkers work on a project instantly and together to create a refined response or product that are of higher quality and more reliable. There are three examples that the paper explores. Namely, Adrenaline (an app that lets workers filter down a video to one single moment to describe the video best), Puppeteer, and A|B. 

Computation within seconds allows crowd workers to get refined feedback within ten seconds ideally. The way to recruit such workers is to hire them in advance, place them on hold for a low cost, and alert them when a task is ready. A|B, for example, asks users which font may look better or which outfit looks best on different models to get instant responses. On Mechanical Turk, requesters recruited workers beforehand to keep them “on call.” When a task was posted, they used Javascript to issue a pop up alert on a worker’s browser. The study found that a crowd could be produced in 2 seconds. Paying a bonus for quick responses was even more effective than the Javascript option, and overall one worker cost $0.30 per hour.

However, it was hard to control feedback quality and speed of completing a task. Algorithmic design helped in narrowing down the options for a task after other workers have completed it. Adrenaline, for example, declares agreement when it detects that many workers are spending time on a specific photo series concurrently. However, this somewhat forces perhaps talented workers to agree to the majority who may be wrong. 

The study suggests that crowds of 2-5 people can pick the best photo, like in Adrenaline’s model, rather than the fastest member of any given group of people. Thus, divide and conquer is really not the best method for crowdsourcing. We now know that we can recruit a crowd in 2 seconds, get them to answer in 5, and complete a complex search in 10 seconds. 

###Description of Mason-Watts “Financial Incentives” Paper

The last paper we reviewed, and the paper we chose to replicate, is “Financial Incentives and the ‘Performance of Crowds’” by Winter Mason and Duncan J. Watts. In their paper, they set out to discover if the quantity and accuracy of crowdworkers’ output varied based on the level of payment they are offered to complete a task. To do this, they set up two experiments: one that required crowdworkers to sort sequential traffic-cam images and one that asked them to complete a word-search puzzle. For each experiment, they varied the difficulty and level of payment. The results of their experiments concluded that while quantity of output (proxied as the number of tasks completed per contributor) increased along with pay-level, there was no significant impact of payment level on accuracy (completing the task correctly). As a possible rationale for the latter result, they suggested an “anchoring effect” in which workers valued their own work relative to the amount of pay they are offered, and thus changes in pay-levels do not incentivize workers to feel any greater obligation to be more accurate. 

###Explanation of Mason-Watts Experimental Design & Results

There were two experiments that were run in this paper. The first was an experiment in which participants had to sort up to 99 sets of images of moving traffic into their correct order. Participants were assigned randomly to one of three difficulty levels and one of three rates of pay. In total, there were 9 different combinations a worker could be assigned to. The second experiment had workers solve up to 24 word puzzles. Each puzzle had them find words in a 2-D grid of letters where they were either paid “per word” or “per puzzle.”

Experiment 1 focused on differentiating between the quantity and quality of work and understanding how financial compensation impacted performance. Each account could only participate once. For the HIT, there was a set of images pulled from a traffic camera that had pictures of traffic in every 2 second interval. Workers had instructions to “Sort images from traffic cameras in chronological order” by dragging and dropping. Some participants were told that they would get an additional bonus on top of the base rate $0.10 for the HIT if they sorted the images. Each worker had to do three tests questions with three images each, and were asked to fill out a demographic survey if they wished. After the practice trial, each worker was randomly assigned to a difficulty level (2 images per set, 3 images, 4 images) with $0.01, $0.05, and $0.10 in increasing difficulty. They were told that there was no reason for any other worker to have a different difficulty or payment level than they had. They could claim the $0.10 bonus at any time from sorting 0 to 99 sets of images. 

611 workers participated with 36,425 image sets sorted. The results found that across all difficulty levels, participants chose to complete tasks that had higher pay. Across all payment levels, the number of tasks completed decreased with increasing difficulty. This is consistent with the economic theory that the more a person is paid to do something, the more they will do it.  Accuracy also did not increase with higher compensation, since the workers presumably thought that they would be paid regardless of their work quality. The paper showed these results in the form of graphs with accuracy or number of tasks completed on the y axis vs. pay per task on the x axis. 

The second experiment stemmed off of the fact that apparently the workers were not any more motivated or accurate even if they were paid more. The authors thought that it might be intrinsic to the task they asked workers to do, since there was only a small number of potential solutions (i.e. sorting). They instead make workers find words in a word puzzle. Each HIT provided a list of words that might be found in the puzzle even though only a subset of them were actually in it (workers did not know this) with an interactive image of the accompanying word puzzle. They also randomly assigned workers to varying compensation levels with two different compensation “tracks” -- one where people were paid by puzzle, and the other for every word found. There was no pay, low, medium, and high pay assigned randomly. Participants also could complete a demographics survey if they accepted the task, and they would select a word in the puzzle by dragging their mouse over the screen. After submitting at least 8 words, they received feedback on how many words they found and could have found. Payment by puzzle or word was then revealed. Participants could complete up to 24 puzzles.

320 workers participated with 2736 puzzles solved and 23,440 words found. No worker restrictions were mentioned. Both experiments had over 80% workers from the US, with an average age in the mid 30s. Participants who were paid either on a per puzzle or per word basis completed more puzzles than those who were not paid. Per puzzle workers were paid got $0.01, $0.05, or $0.10 per puzzle while per word workers got $0.01, $0.02, or $0.03 per word. There was no specific impact on the level of pay vs. quantity of work, differing from the previous experiment. Participants seemed to like the task a lot more, though, with one worker spending nearly 5 hours on tasks that only gave him or her $0.10 at the end as the bonus. Therefore, some found the tasks intrinsically interesting. In addition, the authors found again that money was not really correlated with the quality of performance. Those who weren’t paid anything at all actually found the most words. Those who were being paid per word earned more on average than those who were being paid per puzzle.  A “quota” like structure is more effective, therefore, than a “piece-rate” might be. Workers also had the option to estimate the value of the puzzle or word in each task, and those who were not paid perceived the task to be worth more than those paid the least and less than those paid the most. These results were in a graph that mapped accuracy vs pay (ranging from no pay to the $0.03 and $0.10 highest tiers) and number of puzzles completed vs. pay. In summary, how people are paid largely impacts their output and accuracy than how much they are paid. 

There were two basic conclusions that came out of both experiments. First, paying people to do the work had more results than not paying them at all. For example, in experiment one, paying workers more resulted in a higher output. Secondly, paying workers did not actually affect their accuracy. This is described by the anchoring effect, where workers who are paid more believe their work to be more valuable. They are actually not any more motivated than the workers who were paid less, but just correlate how much money they get with how much value they give their work. Last, the design of how people are paid rather than how much is significant for assessing quality. 

###Why we chose to replicate the first experiment
The study used two experiments: arranging sequences of images and finding words in a word-search. We decided to choose only the first experiment for a few reasons. First, a word-search would take a lot of time for the workers to complete. Because we didn’t have the ability to make an exit poll on quitting the task (the study did this in Amazon’s Mechanical Turk and we used Crowdflower), we didn’t want to risk many people quitting the task in the middle without asking them the exit poll questions. These questions would try to assess how much they believed their time to be worth, compared with how much they actually were being paid. This decision meant that we would not be able to test the anchoring effect in our own experiment. Second, we didn’t feel creating 90 different word-searches was an effective use of our time for this recreation. Third, we thought image ordering was more attainable, since it made no assumptions and created no barriers regarding the workers’ language skills and spelling abilities. Especially knowing that we would need a lot of workers in a somewhat short amount of time, we wanted our task to be very accessible to different kinds of people, and didn’t want to exclude any workers because of preexisting language or spelling barriers.

Instead of the word-search, we decided to take a similar approach to the sequencing of images by having our workers correctly sort numbers in increasing order. Like the experiments in the paper, we had three levels of difficulty (ordering two numbers, ordering three numbers, and ordering four numbers) with three levels of pay each (one cent, three cents, and five cents). We chose ordering numbers because it was easy to write a script to generate two, three, or four random, unequal numbers that can be exported to a CSV and imported to Crowdflower as such. We originally planned to use sequences of images, but ran into quite some trouble in trying to get Crowdflower to randomize them and creating an effective interface for the workers to use. We decided a task similar in simplicity and clarity would be to order numbers. This creates (as well as excludes) the same barriers as does image ordering, like that of language, vision, or education.  

###Experimental setup and deviations from original design
Since we decided to have 90 HITs in each category (easy, medium, or hard), and we wanted the HITs to be accessible to most people (so that no one is excluded based on language ability), we decided to create 90 sequences of random numbers. The level of difficulty would be decided by how many numbers the user needed to sort--easy had to sort two numbers, medium three, and hard four. We wrote a short Python script to generate these random numbers from 0 to 1000, created a CSV out of the results, and then uploaded this to Crowdflower. To test the different levels of pay, we decided each difficulty level would be listed with three different pay levels: one cent, three cents, and five cents. This left us with nine different HIT sections to be posted. To ensure that no workers could work on the same task for a different level of pay, or the same pay but a different difficulty level, we restricted each HIT section to a certain region. This was the only way Crowdflower would allow us to restrict workers. We realize this may have skewed our results in a small way, because workers from different countries may place different values on each level of pay, but we had no way to avoid this because we didn’t have another way to ensure each HIT worker is unique. In our actual HIT design, we posted very clear instructions that listed how we want the worker to structure their input--that is no spaces with the displayed numbers sorted into increasing order. This deviated from the experiment’s original design because they had drag and drop images to be ordered, with an interactive interface. The interactive interface is definitely easier for the worker to use, however we weren’t sure how to create a simple interactive interface for the ordering of numbers. The drag and drop interface also eliminated any reason for typos. However, as our study is investigating the level of effort each worker puts into their HIT, the typos would be essentially the same as a misordering, since it is indicative of the level of effort coming from the worker. Finally, we posted each HIT in a different region and waited for the results.


Figure 1: Screenshot of one of our HITs, this one of Hard difficulty

###Our crowdsourcing platform, worker selection criteria, and payment scheme

Crowdflower was our crowdsourcing platform of choice. Originally the platform was made for tasks that were easier for people than they were for machines. However, it now also functions as a labor market in which “requesters” list human intelligence tasks (HITs) with an accompanied compensation level. Workers can pick any of the tasks that they are qualified for and then are paid for their work. It is one of the most reliable ways to make money on the Internet.

When viewing HITs, workers can see the title of the job, the reward for completing a HIT, and the number of HITs available in that job. They can look at a brief description of the task and also preview it. They can accept it or not, and then begin completing the task if they wish. The time frame for HITs range from seconds to hours, and workers are usually paid $0.01 to $0.10 per HIT. This allows many people to conduct experiments or take surveys with a low barrier to entry in terms of payment and hiring candidates. There are also no distractions on the physical user interface, making the rate of error or lack of completion quite low. 

At the same time, it is hard to say that the population on Crowdflower is representative of the general public because it is a fairly exclusive and unique environment. Anyone can use this tool to randomly assign HITs to people, though, so some sense of diversity within workers is quite possible. Another possible con of Crowdflower is its low rates of payment for each HIT. Participants may not take tasks as seriously as they would in a typical focus group, for example. However, from previous experiments, it seems that workers are very focused on satisfying their HITs requests since poor HIT execution affects their own profile and credibility percentages. 

For our experiment conducted in this paper, the only criteria we had was geographic region for the different HITs we posted. We wanted to make sure that we were not getting the same workers to complete all of our tasks. We assigned a different country or region to each of the 9 HITs, namely: Greece/Italy/Turkey; Indonesia; Mexico; USA; Canada; the United Kingdom; Russia; Germany; India. 3 HITs were $0.01 for the easiest task, 3 HITs were $0.03 for the medium task, and 3 HITs were $0.05 for the hardest task. Greece/Italy/Turkey, Russia, and the USA had the easiest tasks; Indonesia, Canada, and Germany had the medium tasks, and Mexico, the United Kingdom, and India had the hardest tasks. We did not use any other criteria to reject the workers save for these country restrictions. 

###Presentation and Analysis of Results


Figure 2: Summary of Raw Results

####Did quantity of output vary with pay or difficulty?

First we will discuss the quantity of worker output, as measured by the number of sequences they sorted on average. In contrast with the Mason-Watts paper, we found that Average Output per Contributor increased significantly with Difficulty. Normalized across pay-levels, this is surprising as we would expect workers to perform fewer tasks as they got harder.


Figure 3: Workers completed more tasks as they got more difficult

Also surprising is that we found quantity to be declining as the prospective pay-level increased. We would expect that as they are paid more for a given task relative to other available HITs on Crowdflower, workers would choose to complete more of them. Given these initial two findings, we decided to analyze the data closer to find a possible explanation.


Figure 4: Workers completed fewer tasks as they were paid more

After looking at the relationship between Average Number of Unique Contributors for a HIT and the Pay-Level, we found that paying the workers more attracted a greater number of contributors to attempt the tasks. This makes sense, as more workers will be drawn to the greater reward. However, because we had a relatively limited number of tasks to be completed (180-200 per HIT relative to thousands for the Mason-Watts experiment), this increased worker attraction led to a “crowding-out effect.” 


Figure 5: Higher pay-rates attracted significantly more contributors to a HIT

The crowding-out effect can be seen in Figure 6. As the number of unique contributors for a HIT increased, the average output decreases. This is due to the increased number of workers grabbing at a relatively small fixed-sized “pie.” 


Figure 6: Workers’ average output declines as more workers attempt the HIT

####Did accuracy vary with pay or difficulty?

Next, we will see how Accuracy (measured as the % of correctly-ordered sequences) varied with Pay-Level and Difficulty. 
Our first conclusion was that Accuracy was positively correlated with pay-level. This is in stark contrast to the Mason-Watts paper, which found no such correlation due to an “anchoring effect.”


Figure 7: Accuracy improves with pay-level

Our second (surprising) result was that accuracy did not vary predictably with difficulty. While the most accurate HITs were those with the easiest difficulty (as we would predict), we also found that accuracy was greater for the hardest difficulty relative to the medium difficulty level. The reason for this could be that the difficulty levels were not differentiated enough in their difficulty to reveal any significant difference in Accuracy between them, and the much lower accuracy of the medium difficulty HITs could simply be due to random variation in our relatively small sample size. 


Figure 8: Accuracy does not vary predictably with difficulty

###Summary and Discussion of Results

In summary, we first found that quantity of output increased with difficulty and decreased with pay-level. These results oppose the findings of Mason-Watts. The former is difficult to explain, but due to our relatively small sample size (only ~10-30 workers per HIT) it is possible that it is a random anomaly; the latter finding can possibly be explained by the fact that we had a relatively limited supply of tasks for workers to perform, so offering higher pay attracted more contributors, and thus each contributor was forced to complete fewer tasks than they otherwise would have completed. 

Second, we looked at accuracy and found that while it did not vary significantly with difficulty, it did increase proportionally with pay. Both of these findings are in contrast to the Mason-Watts findings, which supposed accuracy to vary inversely with difficulty and not significantly with pay. One likely explanation for why accuracy did not depend on difficulty is that the difficulty levels were not significantly different in their difficulty (i.e., sorting four numbers is not too much more difficult than sorting two numbers). Perhaps accuracy varied with pay because of a random anomaly due to small sample size, or because there was an absence of the “anchoring problem” Mason-Watts found, as we did not emphasize the payment levels as much as they did. 



